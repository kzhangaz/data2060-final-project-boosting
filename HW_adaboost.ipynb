{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def node_score_error(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the train error of the subdataset and return it.\n",
    "        For a dataset with two classes, C(p) = min{p, 1-p}\n",
    "    '''\n",
    "    return min(prob, 1.0 - prob)\n",
    "\n",
    "def node_score_entropy(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the entropy of the subdataset and return it.\n",
    "        For a dataset with 2 classes, C(p) = -p * log(p) - (1-p) * log(1-p)\n",
    "        For the purposes of this calculation, assume 0*log0 = 0.\n",
    "        HINT: remember to consider the range of values that p can take!\n",
    "    '''\n",
    "    # HINT: If p < 0 or p > 1 then entropy = 0\n",
    "\n",
    "    if prob <= 0.0 or prob >= 1.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return -prob * np.log(prob) - (1.0 - prob) * np.log(1.0 - prob)\n",
    "\n",
    "\n",
    "def node_score_gini(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the gini index of the subdataset and return it.\n",
    "        For dataset with 2 classes, C(p) = 2 * p * (1-p)\n",
    "    '''\n",
    "\n",
    "    return 2.0 * prob * (1.0 - prob)\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.thresold = None\n",
    "        self.info = {} # used for visualization\n",
    "\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        You do not need to modify this. \n",
    "        '''\n",
    "\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, gain_function=node_score_entropy, max_depth=1, sample_weight=None):\n",
    "        converted_data = []\n",
    "        for row in data:\n",
    "            new_row = row.copy()\n",
    "            new_row[0] = 1 if row[0] == 1 else 0  # Convert -1 to 0\n",
    "            converted_data.append(new_row)\n",
    "       \n",
    "        # Find majority class (now using 0/1)\n",
    "        self.majority_class = 1 if sum(row[0] for row in converted_data) > len(converted_data) / 2 else 0\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label=self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "        self._split_recurs(self.root, converted_data, indices, sample_weight)\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        internal_pred = self._predict_recurs(self.root, features)\n",
    "        return 1 if internal_pred == 1 else -1  # Convert 0 to -1 for output\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        cnt = 0.0\n",
    "        test_Y = [row[0] for row in data]\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data[i])\n",
    "            if (prediction != test_Y[i]):\n",
    "                cnt += 1.0\n",
    "        return cnt/len(data)\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        '''\n",
    "        Modified to use thresholds for splitting\n",
    "        '''\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "\n",
    "        if row[node.index_split_on] <= node.threshold:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "                    \n",
    "        \n",
    "    def _is_terminal(self, node, data, indices, sample_weight=None):\n",
    "        '''\n",
    "        TODO:\n",
    "        Helper function to determine whether the node should stop splitting.\n",
    "        Stop the recursion if:\n",
    "            1. The dataset (as passed to parameter data) is empty.\n",
    "            2. There are no more indices to split on.\n",
    "            3. All the instances in this dataset belong to the same class\n",
    "            4. The depth of the node reaches the maximum depth.\n",
    "        Set the node label to be the majority label of the training dataset if:\n",
    "            1. The number of class 1 points is equal to the number of class 0 points.\n",
    "            2. The dataset is empty.\n",
    "        Return:\n",
    "            - A boolean, True indicating the current node should be a leaf and \n",
    "              False if the node is not a leaf.\n",
    "            - A label, indicating the label of the leaf (or the label the node would \n",
    "              be if we were to terminate at that node). If there is no data left, you\n",
    "              must return the majority class of the training set.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "\n",
    "        # Check Cases if the node should stop splitting\n",
    "        sumy = np.sum(sample_weight[y == 1]) if sample_weight is not None else sum(row[0] for row in data)\n",
    "\n",
    "        majority_label = 1 if sumy > 0 else 0\n",
    "            \n",
    "\n",
    "        if len(set(y)) == 1:\n",
    "            return True, y[0]\n",
    "        if len(data) == 0:\n",
    "            return True, self.majority_class\n",
    "        if len(indices) == 0:\n",
    "            return True, majority_label\n",
    "        \n",
    "        \n",
    "        # TODO: Check cases if the node should be set to the majority label of the training dataset\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True, majority_label\n",
    "\n",
    "        return False, majority_label\n",
    "        \n",
    "\n",
    "    def _split_recurs(self, node, data, indices, sample_weight=None):\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(data)) / len(data)\n",
    "\n",
    "        node.isleaf, node.label = self._is_terminal(node, data, indices, sample_weight)\n",
    "\n",
    "        if not node.isleaf:\n",
    "            max_gain = -1\n",
    "            best_threshold = None\n",
    "\n",
    "            for split_index in indices:\n",
    "                feature_values = sorted(set(row[split_index] for row in data))\n",
    "\n",
    "                for i in range(len(feature_values) - 1):\n",
    "                    threshold = (feature_values[i] + feature_values[i + 1]) / 2\n",
    "                    gain = self._calc_gain(data, split_index, self.gain_function, threshold, sample_weight)\n",
    "\n",
    "                    if gain > max_gain:\n",
    "                        max_gain = gain\n",
    "                        node.index_split_on = split_index\n",
    "                        best_threshold = threshold\n",
    "\n",
    "            node._set_info(max_gain, len(data))\n",
    "            node.threshold = best_threshold\n",
    "\n",
    "            node.left = Node(depth=node.depth + 1)\n",
    "            node.right = Node(depth=node.depth + 1)\n",
    "            indices.remove(node.index_split_on)\n",
    "\n",
    "            leftData = [row for row in data if row[node.index_split_on] <= node.threshold]\n",
    "            rightData = [row for row in data if row[node.index_split_on] > node.threshold]\n",
    "\n",
    "            left_weights = sample_weight[[row[node.index_split_on] <= node.threshold for row in data]]\n",
    "            right_weights = sample_weight[[row[node.index_split_on] > node.threshold for row in data]]\n",
    "\n",
    "            self._split_recurs(node.left, leftData, indices, left_weights)\n",
    "            self._split_recurs(node.right, rightData, indices, right_weights)\n",
    "        else:\n",
    "            node._set_info(0, len(data))\n",
    "\n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function, threshold, sample_weight):\n",
    "        '''\n",
    "        Calculate the gain of the proposed splitting and return it.\n",
    "        Gain = C(P[y=1]) - P[x_i=True] * C(P[y=1|x_i=True]) - P[x_i=False] * C(P[y=0|x_i=False])\n",
    "        Here the C(p) is the gain_function. For example, if C(p) = min(p, 1-p), this would be\n",
    "        considering training error gain. Other alternatives are entropy and gini functions.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [1 if row[split_index] > threshold else 0 for row in data]\n",
    "\n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "            # Calculate weighted probabilities\n",
    "            probY = np.sum(sample_weight[y == 1]) / np.sum(sample_weight)\n",
    "            probX = np.sum(sample_weight[xi == 1]) / np.sum(sample_weight)\n",
    "\n",
    "            y1x1 = np.sum(sample_weight[(y == 1) & (xi == 1)])\n",
    "            y0x0 = np.sum(sample_weight[(y == 0) & (xi == 0)])\n",
    "\n",
    "            prob1 = y1x1 / np.sum(sample_weight)\n",
    "            prob2 = y0x0 / np.sum(sample_weight)\n",
    "\n",
    "            if abs(probX - 0) < 1e-10:\n",
    "                probxi_true = 0\n",
    "            else:\n",
    "                probxi_true = probX * gain_function(prob1 / probX)\n",
    "\n",
    "            if abs(probX - 1.0) < 1e-10:\n",
    "                probxi_false = 0.0\n",
    "            else:\n",
    "                probxi_false = (1.0 - probX) * gain_function(prob2 / (1.0 - probX))\n",
    "\n",
    "            gain = gain_function(probY) - probxi_true - probxi_false\n",
    "            \n",
    "        else:\n",
    "            gain = 0\n",
    "\n",
    "        return gain\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree2:\n",
    "    def __init__(self, data, gain_function=None, max_depth=40):\n",
    "        converted_data = []\n",
    "        for row in data:\n",
    "            new_row = row.copy()\n",
    "            new_row[0] = 1 if row[0] == 1 else 0  # Convert -1 to 0\n",
    "            converted_data.append(new_row)\n",
    "\n",
    "        self.majority_class = 1 if sum(row[0] for row in converted_data) > len(converted_data) / 2 else 0\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label=self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "        self._split_recurs(self.root, converted_data, indices)\n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        node.isleaf, node.label = self._is_terminal(node, data, indices)\n",
    "\n",
    "        if not node.isleaf:\n",
    "            max_gain = -1\n",
    "            best_threshold = None\n",
    "\n",
    "            for split_index in indices:\n",
    "                feature_values = sorted(set(row[split_index] for row in data))\n",
    "\n",
    "                for i in range(len(feature_values) - 1):\n",
    "                    threshold = (feature_values[i] + feature_values[i + 1]) / 2\n",
    "                    gain = self._calc_gain(data, split_index, self.gain_function, threshold)\n",
    "\n",
    "                    if gain > max_gain:\n",
    "                        max_gain = gain\n",
    "                        node.index_split_on = split_index\n",
    "                        best_threshold = threshold\n",
    "\n",
    "            node._set_info(max_gain, len(data))\n",
    "            node.threshold = best_threshold\n",
    "\n",
    "            node.left = Node(depth=node.depth + 1)\n",
    "            node.right = Node(depth=node.depth + 1)\n",
    "            indices.remove(node.index_split_on)\n",
    "\n",
    "            leftData = [row for row in data if row[node.index_split_on] <= node.threshold]\n",
    "            rightData = [row for row in data if row[node.index_split_on] > node.threshold]\n",
    "\n",
    "            self._split_recurs(node.left, leftData, indices)\n",
    "            self._split_recurs(node.right, rightData, indices)\n",
    "\n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        y = [row[0] for row in data]\n",
    "\n",
    "        if len(set(y)) == 1:\n",
    "            return True, y[0]\n",
    "        if len(data) == 0:\n",
    "            return True, self.majority_class\n",
    "        if len(indices) == 0:\n",
    "            return True, max(set(y), key=y.count)\n",
    "\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True, max(set(y), key=y.count)\n",
    "\n",
    "        return False, None\n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function, threshold):\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [1 if row[split_index] > threshold else 0 for row in data]\n",
    "\n",
    "        # Calculate gain based on the chosen gain function\n",
    "        # This is a placeholder; you should implement the actual gain calculation logic\n",
    "        gain = gain_function(y, xi) if gain_function else 0\n",
    "        return gain\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Helper function to predict the label given a row of features.'''\n",
    "        node = self.root\n",
    "        while not node.isleaf:\n",
    "            if features[node.index_split_on] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW_adaboost.py\n",
    "from sklearn.tree import DecisionTreeClassifier  # Importing DecisionTreeClassifier\n",
    "    \n",
    "class AdaBoostClassifier:\n",
    "    \"\"\"\n",
    "    AdaBoost (Adaptive Boosting) Classifier\n",
    "    An ensemble learning algorithm that combines multiple weak classifiers to build a strong classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=2):\n",
    "        \"\"\"\n",
    "        Initialize the AdaBoost classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - n_estimators: Number of weak classifiers to use.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth  # Store max_depth for DecisionTree\n",
    "        self.w = []  # Store the weights of the classifiers\n",
    "        self.models = []  # Store the weak classifiers\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the AdaBoost model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Training data, shape (n_samples, n_features)\n",
    "        - y: Target labels, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights uniformly\n",
    "        D = np.ones(n_samples) / n_samples  \n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            \n",
    "            # sklearn\n",
    "            # Create a weak classifier (decision stump)\n",
    "            model = DecisionTreeClassifier(max_depth=2)  \n",
    "            # Fit the model to the training data\n",
    "            model.fit(X, y, sample_weight=D)  # Add this line to train the model\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred1 = model.predict(X)  # Update to use the model's predict method\n",
    "            print(\"y_pred1: \", y_pred1)\n",
    "           \n",
    "            \n",
    "            \n",
    "            # self-implemented\n",
    "            weak_model = DecisionTree(data=np.column_stack((y, X)), max_depth=self.max_depth) \n",
    "            y_pred = np.zeros_like(y)  # Initialize y_pred\n",
    "            for i, (y_i, x_i) in enumerate(zip(y, X)):\n",
    "                combined_input = np.append(y_i, x_i)\n",
    "                y_pred_i = weak_model.predict(combined_input)  # Predictions from the model\n",
    "                y_pred[i] = y_pred_i  # Update y_pred with the prediction\n",
    "            print(\"y_pred: \", y_pred)\n",
    "            \n",
    "            weak_model2 = DecisionTree2(data=np.column_stack((y, X)), max_depth=self.max_depth) \n",
    "            y_pred2 = np.zeros_like(y)  # Initialize y_pred\n",
    "            for i, (y_i, x_i) in enumerate(zip(y, X)):\n",
    "                combined_input = np.append(y_i, x_i)\n",
    "                y_pred_i = weak_model2.predict(combined_input)  # Predictions from the model\n",
    "                y_pred2[i] = y_pred_i  # Update y_pred with the prediction\n",
    "            print(\"y_pred2: \", y_pred2)\n",
    "            \n",
    "\n",
    "            # Calculate the weighted error\n",
    "            error = np.sum(D * (y_pred != y))  # Weighted error\n",
    "\n",
    "            # Error cannot be exactly 0.5 because it represents the weighted sum of misclassifications.\n",
    "            # If error is 0.5, it means the model is performing no better than random guessing,\n",
    "            # This means the model is not contributing to the ensemble learning process,\n",
    "            # and the weights D will not be updated, leading to no improvement in the model.\n",
    "            if error == 0.5:\n",
    "                print(\"Warning: Error is 0.5, stopping training.\")\n",
    "                break\n",
    "\n",
    "            # Calculate the weight for the weak classifier\n",
    "            w_t = 0.5 * np.log((1.0 - error) / (error + 1e-10))  # Avoid division by zero\n",
    "\n",
    "            # Update weights for the next iteration\n",
    "            D *= np.exp(-w_t * y * y_pred)  # Update weights based on prediction\n",
    "            D /= np.sum(D * np.exp(-w_t * y * y_pred))  # Normalize weights\n",
    "\n",
    "\n",
    "            self.models.append(weak_model)  # Store the model\n",
    "            self.w.append(w_t)  # Store the w_t\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data, shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "        - Predicted class labels, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        pred = np.zeros(X.shape[0])  # Initialize predictions\n",
    "        for w_i, model in zip(self.w, self.models):\n",
    "            pred += w_i * model.predict(X)  # Weighted sum of predictions\n",
    "        return np.sign(pred)  # Return the sign of the predictions\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data, shape (n_samples, n_features)\n",
    "        - y: True labels, shape (n_samples,)\n",
    "\n",
    "        Returns:\n",
    "        - Accuracy as a float.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)  # Get predictions\n",
    "        accuracy = np.mean(predictions == y)  # Calculate accuracy\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred1:  [-1 -1 -1 -1 -1  1]\n",
      "y_pred:  [-1 -1 -1 -1 -1 -1]\n",
      "y_pred2:  [0 1 1 0 0 1]\n",
      "Warning: Error is 0.5, stopping training.\n",
      "Predictions: [0. 0. 0. 0. 0. 0.]\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     # Create a simple dataset\n",
    "    X = np.array([[0, 0], [1, 1], [1, 0], [0, 1], [0, 2], [1, 2]])\n",
    "    y = np.array([-1, -1, 1, 1, -1, 1])  # Labels should be -1 and 1 for AdaBoost\n",
    "\n",
    "    # Initialize the AdaBoost classifier\n",
    "    model = AdaBoostClassifier(n_estimators=10, max_depth=2)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = model.accuracy(X, y)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1  1  1]\n",
      "y_pred:  [-1 -1 -1  1]\n",
      "y_pred2:  [0 0 0 1]\n",
      "y_pred1:  [-1 -1 -1 -1 -1  1]\n",
      "y_pred:  [-1 -1 -1 -1 -1 -1]\n",
      "y_pred2:  [0 1 1 0 0 1]\n",
      "Warning: Error is 0.5, stopping training.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Model should have trained at least one weak learner.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m check_train_dtype(test_model1, x1, y1)\n\u001b[0;32m     32\u001b[0m test_model2\u001b[38;5;241m.\u001b[39mtrain(x2, y2)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mcheck_train_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_model2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m test_model3\u001b[38;5;241m.\u001b[39mtrain(x3, y3)\n\u001b[0;32m     36\u001b[0m check_train_dtype(test_model3, x3, y3)\n",
      "Cell \u001b[1;32mIn[120], line 25\u001b[0m, in \u001b[0;36mcheck_train_dtype\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_train_dtype\u001b[39m(model, X, y):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodels) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel should have trained at least one weak learner.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mw) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodels), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights should match the number of models.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Model should have trained at least one weak learner."
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "test_model1 = AdaBoostClassifier(n_estimators=10)\n",
    "test_model2 = AdaBoostClassifier(n_estimators=50)\n",
    "test_model3 = AdaBoostClassifier(n_estimators=20)\n",
    "\n",
    "# Creates Test Data\n",
    "x1 = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "y1 = np.array([-1, -1, 1, 1])  # Labels should be -1 and 1 for AdaBoost\n",
    "\n",
    "x2 = np.array([[0, 0], [1, 1], [1, 0], [0, 1], [0, 2], [1, 2]])\n",
    "y2 = np.array([-1, -1, 1, 1, -1, 1])  # More complex dataset\n",
    "\n",
    "x3 = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "y3 = np.array([-1, -1, 1, 1, 1, 1])  # Another dataset\n",
    "\n",
    "# Test Model Train\n",
    "def check_train_dtype(model, X, y):\n",
    "    assert isinstance(model.models, list)\n",
    "    assert len(model.models) > 0, \"Model should have trained at least one weak learner.\"\n",
    "    assert len(model.w) == len(model.models), \"Weights should match the number of models.\"\n",
    "\n",
    "# Train the models\n",
    "test_model1.train(x1, y1)\n",
    "check_train_dtype(test_model1, x1, y1)\n",
    "\n",
    "test_model2.train(x2, y2)\n",
    "check_train_dtype(test_model2, x2, y2)\n",
    "\n",
    "test_model3.train(x3, y3)\n",
    "check_train_dtype(test_model3, x3, y3)\n",
    "\n",
    "# Test Model Predictions\n",
    "def check_test_dtype(pred, X_test):\n",
    "    assert isinstance(pred, np.ndarray)\n",
    "    assert pred.ndim == 1 and pred.shape == (X_test.shape[0],)\n",
    "\n",
    "# Make predictions\n",
    "pred1 = test_model1.predict(x1)\n",
    "check_test_dtype(pred1, x1)\n",
    "assert (pred1 == y1).all(), \"Predictions should match the expected labels for model 1.\"\n",
    "\n",
    "pred2 = test_model2.predict(x2)\n",
    "check_test_dtype(pred2, x2)\n",
    "assert (pred2 == y2).all(), \"Predictions should match the expected labels for model 2.\"\n",
    "\n",
    "pred3 = test_model3.predict(x3)\n",
    "check_test_dtype(pred3, x3)\n",
    "assert (pred3 == y3).all(), \"Predictions should match the expected labels for model 3.\"\n",
    "\n",
    "# Test Model Accuracy\n",
    "def check_accuracy(model, X, y, expected_accuracy):\n",
    "    accuracy = model.accuracy(X, y)\n",
    "    assert accuracy == expected_accuracy, f\"Expected accuracy: {expected_accuracy}, but got: {accuracy}\"\n",
    "\n",
    "# Check accuracy\n",
    "check_accuracy(test_model1, x1, y1, 1.0)  # Expecting 100% accuracy for this simple case\n",
    "check_accuracy(test_model2, x2, y2, 1.0)  # Expecting 100% accuracy for this dataset\n",
    "check_accuracy(test_model3, x3, y3, 1.0)  # Expecting 100% accuracy for this dataset\n",
    "\n",
    "# Additional Tests for Edge Cases\n",
    "def test_empty_train():\n",
    "    with pytest.raises(ValueError):\n",
    "        test_model1.train(np.array([]), np.array([]))\n",
    "\n",
    "def test_empty_predict():\n",
    "    with pytest.raises(ValueError):\n",
    "        test_model1.predict(np.array([]))\n",
    "\n",
    "def test_accuracy_empty():\n",
    "    with pytest.raises(ValueError):\n",
    "        test_model1.accuracy(np.array([]), np.array([]))\n",
    "\n",
    "# Run additional edge case tests\n",
    "test_empty_train()\n",
    "test_empty_predict()\n",
    "test_accuracy_empty()\n",
    "\n",
    "# Print a message indicating the tests have completed\n",
    "print(\"All tests completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
