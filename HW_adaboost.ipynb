{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一些情况说明：\n",
    "- adaboost 的参数命名是基于我们的readme file\n",
    "- 有关决策树 - 根据hw05改的，有以下几个改变：\n",
    "    - adaboost 的结果一般是-1， 1； decision tree是0，1.所以设置了converted 这个变量， 如果 = true， 那么-1， 1 与0， 1 相互转换\n",
    "    - 针对决策树增加了weight\n",
    "    - 针对决策树增加了threshold. 本意是这样数据集X不光可以是0/1， 也可以是别的数字。但是！现在！过不了！（TODO！！）\n",
    "    - 在adaboost调用decision tree的时候，predict时，针对X每行开头增加了一个0，变为X_with_zero。因为decision tree的predict传入参数是[y+x]的形式\n",
    "- 一些代码调试相关\n",
    "    - decisiontree.ipynb 是decision tree单独拎出来的。目前可以过hw05的所有test。（估计过不了input有0/1以外的test)\n",
    "    - adaboost.ipynb 注释掉了一段调用decision tree标准库的代码，可以用那段代码作为参照 - 当调用那段代码的时候，X的input可以是任意形式。但是需要注意，如果使用这段代码， adaboost的predict函数不要用X_with_zero， 要用X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def node_score_error(prob):\n",
    "    '''\n",
    "        Calculate the node score using the train error of the subdataset and return it.\n",
    "        For a dataset with two classes, C(p) = min{p, 1-p}\n",
    "    '''\n",
    "    return min(prob, 1.0 - prob)\n",
    "\n",
    "def node_score_entropy(prob):\n",
    "    '''\n",
    "        Calculate the node score using the entropy of the subdataset and return it.\n",
    "        For a dataset with 2 classes, C(p) = -p * log(p) - (1-p) * log(1-p)\n",
    "        For the purposes of this calculation, assume 0*log0 = 0.\n",
    "        HINT: remember to consider the range of values that p can take!\n",
    "    '''\n",
    "    # HINT: If p < 0 or p > 1 then entropy = 0\n",
    "\n",
    "    if prob <= 0.0 or prob >= 1.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return -prob * np.log(prob) - (1.0 - prob) * np.log(1.0 - prob)\n",
    "\n",
    "\n",
    "def node_score_gini(prob):\n",
    "    '''\n",
    "        Calculate the node score using the gini index of the subdataset and return it.\n",
    "        For dataset with 2 classes, C(p) = 2 * p * (1-p)\n",
    "    '''\n",
    "\n",
    "    return 2.0 * prob * (1.0 - prob)\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {}  # used for visualization\n",
    "        self.threshold = None\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        '''\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, gain_function=node_score_entropy, max_depth=40, weight=None, converted=None):\n",
    "        # Initialize the decision tree with data and parameters.\n",
    "        if converted is not None:\n",
    "            for row in data:\n",
    "                if row[0] == -1:\n",
    "                    row[0] = 0  # Convert -1 to 0\n",
    "                    \n",
    "        self.majority_class = 1 if sum(row[0] for row in data) > len(data) / 2 else 0\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label=self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "        if weight is None:\n",
    "            self.sample_weight = np.ones(len(data)) / len(data)\n",
    "        else:\n",
    "            self.sample_weight = weight / np.sum(weight)\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "        self._split_recurs(self.root, data, indices, self.sample_weight)\n",
    "\n",
    "\n",
    "    def predict(self, features, converted=None):\n",
    "        '''\n",
    "        Predict the label for given features.\n",
    "        '''\n",
    "        if features.ndim == 1:  # 1d array\n",
    "            prediction = self._predict_recurs(self.root, features)\n",
    "            return -1 if converted and prediction == 0 else prediction\n",
    "        else:  # 2d array\n",
    "            predictions = []\n",
    "            for feature in features:\n",
    "                prediction = self._predict_recurs(self.root, feature)\n",
    "                if converted and prediction == 0:\n",
    "                    prediction = -1\n",
    "                predictions.append(prediction)\n",
    "            return np.array(predictions) \n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Calculate accuracy on the given data.\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Calculate loss on the given data.\n",
    "        '''\n",
    "        test_Y = np.array([row[0] for row in data])  # Get the true labels\n",
    "        predictions = self.predict(np.array(data))  # Get the predicted results\n",
    "        return np.mean(predictions != test_Y) \n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        '''\n",
    "        Predict label by traversing the tree.\n",
    "        '''\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "        split_index = node.index_split_on\n",
    "        if not row[split_index]:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "\n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        '''\n",
    "        Check if the node should stop splitting.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "\n",
    "        sumy = sum(row[0] for row in data)\n",
    "\n",
    "        if len(data) - sumy == sumy:\n",
    "            majority_label = self.majority_class\n",
    "        else:\n",
    "            majority_label = 1 if sumy > len(data) / 2 else 0\n",
    "\n",
    "        if len(set(y)) == 1:\n",
    "            return True, y[0]\n",
    "        if len(data) == 0:\n",
    "            return True, self.majority_class\n",
    "        if len(indices) == 0:\n",
    "            return True, majority_label\n",
    "\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True, majority_label\n",
    "\n",
    "        return False, majority_label\n",
    "\n",
    "    def _split_recurs(self, node, data, indices, weights):\n",
    "        '''\n",
    "        Recursively split the node based on data.\n",
    "        '''\n",
    "        node.isleaf, node.label = self._is_terminal(node, data, indices)\n",
    "\n",
    "        if not node.isleaf:\n",
    "            max_gain = -1\n",
    "            best_threshold = None\n",
    "\n",
    "            for split_index in indices:\n",
    "                feature_values = sorted(set(row[split_index] for row in data))\n",
    "\n",
    "                for i in range(len(feature_values) - 1):\n",
    "                    threshold = (feature_values[i] + feature_values[i + 1]) / 2\n",
    "                    gain = self._calc_gain(data, split_index, self.gain_function, threshold, weights)\n",
    "\n",
    "                    if gain > max_gain:\n",
    "                        max_gain = gain\n",
    "                        node.index_split_on = split_index\n",
    "                        best_threshold = threshold\n",
    "\n",
    "                if len(feature_values) == 1:\n",
    "                    gain = self._calc_gain(data, split_index, self.gain_function, feature_values[0], weights)\n",
    "                    if gain > max_gain:\n",
    "                        max_gain = gain\n",
    "                        node.index_split_on = split_index\n",
    "                        best_threshold = feature_values[0]\n",
    "\n",
    "            node._set_info(max_gain, len(data))\n",
    "            node.threshold = best_threshold\n",
    "\n",
    "            node.left = Node(depth=node.depth + 1)\n",
    "            node.right = Node(depth=node.depth + 1)\n",
    "            indices.remove(node.index_split_on)\n",
    "\n",
    "            leftData = [row for row in data if row[node.index_split_on] <= node.threshold]\n",
    "            rightData = [row for row in data if row[node.index_split_on] > node.threshold]\n",
    "\n",
    "            left_weights = weights[[row[node.index_split_on] <= node.threshold for row in data]]\n",
    "            right_weights = weights[[row[node.index_split_on] > node.threshold for row in data]]\n",
    "\n",
    "            self._split_recurs(node.left, leftData, indices, left_weights)\n",
    "            self._split_recurs(node.right, rightData, indices, right_weights)\n",
    "        else:\n",
    "            node._set_info(0, len(data))\n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function, threshold=None, weights=None):\n",
    "        '''\n",
    "        Calculate gain for the proposed split.\n",
    "        '''\n",
    "        if threshold is None:\n",
    "            threshold = 0.5\n",
    "        if weights is None:\n",
    "            weights = np.ones(len(data))  # Default weights\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [1 if row[split_index] > threshold else 0 for row in data]\n",
    "        \n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "            total_weight = np.sum(weights)\n",
    "            probY = np.sum(weights * y) / total_weight\n",
    "            probX = np.sum(weights * xi) / total_weight\n",
    "\n",
    "            y1x1 = sum(weights[index] for index in range(len(y)) if y[index] == 1 and xi[index] == 1)\n",
    "            y0x0 = sum(weights[index] for index in range(len(y)) if y[index] == 0 and xi[index] == 0)\n",
    "\n",
    "            prob1 = y1x1 / total_weight \n",
    "            prob2 = y0x0 / total_weight \n",
    "\n",
    "            probxi_true = (probX * gain_function(prob1 / probX)) if probX > 0 else 0\n",
    "            probxi_false = ((1.0 - probX) * gain_function(prob2 / (1.0 - probX))) if probX < 1.0 else 0\n",
    "\n",
    "            gain = gain_function(probY) - probxi_true - probxi_false\n",
    "        else:\n",
    "            gain = 0\n",
    "\n",
    "        return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW_adaboost.py\n",
    "from sklearn.tree import DecisionTreeClassifier  # Importing DecisionTreeClassifier\n",
    "    \n",
    "class AdaBoostClassifier:\n",
    "    \"\"\"\n",
    "    AdaBoost (Adaptive Boosting) Classifier\n",
    "    An ensemble learning algorithm that combines multiple weak classifiers to build a strong classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=1):\n",
    "        \"\"\"\n",
    "        Initialize the AdaBoost classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - n_estimators: Number of weak classifiers to use.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth  # Store max_depth for DecisionTree\n",
    "        self.w = []  # Store the weights of the classifiers\n",
    "        self.models = []  # Store the weak classifiers\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the AdaBoost model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Training data, shape (n_samples, n_features)\n",
    "        - y: Target labels, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights uniformly\n",
    "        D = np.ones(n_samples) / n_samples  \n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            '''\n",
    "            # sklearn\n",
    "            # Create a weak classifier (decision stump)\n",
    "            std_model = DecisionTreeClassifier(max_depth=2)  \n",
    "            # Fit the model to the training data\n",
    "            std_model.fit(X, y, sample_weight=D)  # Add this line to train the model\n",
    "            y_pred_sklearn = std_model.predict(X)\n",
    "            '''\n",
    "            # self-implemented\n",
    "            weak_model = DecisionTree(data=np.column_stack((y, X)), max_depth=self.max_depth, weight=D, converted=True) \n",
    "            y_pred = weak_model.predict(features=np.column_stack((y, X)), converted = True)\n",
    "            # Calculate the weighted error\n",
    "            error = np.sum(D * (y_pred != y))  # Weighted error\n",
    "\n",
    "            # Calculate the weight for the weak classifier\n",
    "            w_t = 0.5 * np.log((1.0 - error) / (error + 1e-10))  # Avoid division by zero\n",
    "\n",
    "            # Update weights for the next iteration\n",
    "            D *= np.exp(-w_t * y * y_pred)  # Update weights based on prediction\n",
    "            D /= np.sum(D * np.exp(-w_t * y * y_pred))  # Normalize weights\n",
    "\n",
    "\n",
    "            self.models.append(weak_model)  # Store the model\n",
    "            self.w.append(w_t)  # Store the w_t\n",
    "\n",
    "    def predict(self, X, converted = True):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data, shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "        - Predicted class labels, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        pred = np.zeros(X.shape[0])  # Initialize predictions\n",
    "        X_with_zero = np.insert(X, 0, 0, axis=1)  # Insert 0 at the beginning of each row\n",
    "        for w_i, model in zip(self.w, self.models):\n",
    "            pred += w_i * model.predict(X_with_zero, converted)  # Weighted sum of predictions\n",
    "        return np.sign(pred)  # Return the sign of the predictions\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data, shape (n_samples, n_features)\n",
    "        - y: True labels, shape (n_samples,)\n",
    "\n",
    "        Returns:\n",
    "        - Accuracy as a float.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)  # Get predictions\n",
    "        accuracy = np.mean(predictions == y)  # Calculate accuracy\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     # Create a simple dataset\n",
    "     X = np.array([\n",
    "        [0, 0, 1, 0],\n",
    "        [1, 1, 0, 1],\n",
    "        [1, 0, 1, 0],\n",
    "        [0, 1, 0, 1],\n",
    "        [0, 0, 0, 0],\n",
    "        [1, 1, 1, 1],\n",
    "        [0, 1, 1, 0],\n",
    "        [1, 0, 0, 1],\n",
    "        [1, 1, 0, 0],\n",
    "        [0, 0, 1, 1]\n",
    "    ])  # 10 samples with 4 features\n",
    "\n",
    "     y = np.array([-1, 1, 1, -1, -1, 1, 1, -1, 1, -1])  # Binary labels (-1 and 1)\n",
    "     # Initialize the AdaBoost classifier\n",
    "     model = AdaBoostClassifier(n_estimators=10, max_depth=1)\n",
    "\n",
    "     # Train the model\n",
    "     model.train(X, y)\n",
    "\n",
    "     # Calculate accuracy\n",
    "     accuracy = model.accuracy(X, y)\n",
    "\n",
    "     # Print results\n",
    "     print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "test_model1 = AdaBoostClassifier(n_estimators=10)\n",
    "test_model2 = AdaBoostClassifier(n_estimators=50)\n",
    "test_model3 = AdaBoostClassifier(n_estimators=20)\n",
    "\n",
    "# Dataset 1\n",
    "x1 = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [1, 1, 0, 1],\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 1, 1, 1],\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [1, 1, 0, 0],\n",
    "    [0, 0, 1, 1]\n",
    "])  # 10 samples with 4 features\n",
    "\n",
    "y1 = np.array([-1, 1, 1, -1, -1, 1, 1, -1, 1, -1])  # Binary labels (-1 and 1)\n",
    "\n",
    "# Dataset 2\n",
    "x2 = np.array([\n",
    "    [0, 1, 0, 1, 1, 0],\n",
    "    [1, 0, 1, 0, 0, 1],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 1, 1, 1, 0],\n",
    "    [0, 1, 0, 0, 1, 1]\n",
    "])  # 10 samples with 6 features\n",
    "\n",
    "y2 = np.array([-1, 1, 1, -1, 1, -1, 1, -1, 1, -1])  # Binary labels (-1 and 1)\n",
    "\n",
    "# Dataset 3\n",
    "x3 = np.array([\n",
    "    [1, 1, 0, 0, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 1, 1, 0],\n",
    "    [0, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0]\n",
    "])  # 10 samples with 6 features\n",
    "\n",
    "y3 = np.array([1, -1, 1, -1, 1, -1, 1, -1, 1, -1])  # Binary labels (-1 and 1)\n",
    "\n",
    "# Test Model Train\n",
    "def check_train_dtype(model, X, y):\n",
    "    assert isinstance(model.models, list)\n",
    "    assert len(model.models) > 0, \"Model should have trained at least one weak learner.\"\n",
    "    assert len(model.w) == len(model.models), \"Weights should match the number of models.\"\n",
    "\n",
    "# Train the models\n",
    "test_model1.train(x1, y1)\n",
    "check_train_dtype(test_model1, x1, y1)\n",
    "\n",
    "test_model2.train(x2, y2)\n",
    "check_train_dtype(test_model2, x2, y2)\n",
    "\n",
    "test_model3.train(x3, y3)\n",
    "check_train_dtype(test_model3, x3, y3)\n",
    "\n",
    "# Test Model Predictions\n",
    "def check_test_dtype(pred, X_test):\n",
    "    assert isinstance(pred, np.ndarray)\n",
    "    assert pred.ndim == 1 and pred.shape == (X_test.shape[0],)\n",
    "\n",
    "# Make predictions\n",
    "pred1 = test_model1.predict(x1)\n",
    "check_test_dtype(pred1, x1)\n",
    "assert (pred1 == y1).all(), \"Predictions should match the expected labels for model 1.\"\n",
    "\n",
    "pred2 = test_model2.predict(x2)\n",
    "check_test_dtype(pred2, x2)\n",
    "assert (pred2 == y2).all(), \"Predictions should match the expected labels for model 2.\"\n",
    "\n",
    "pred3 = test_model3.predict(x3)\n",
    "check_test_dtype(pred3, x3)\n",
    "assert (pred3 == y3).all(), \"Predictions should match the expected labels for model 3.\"\n",
    "\n",
    "# Test Model Accuracy\n",
    "def check_accuracy(model, X, y, expected_accuracy):\n",
    "    accuracy = model.accuracy(X, y)\n",
    "    assert accuracy == expected_accuracy, f\"Expected accuracy: {expected_accuracy}, but got: {accuracy}\"\n",
    "\n",
    "# Check accuracy\n",
    "check_accuracy(test_model1, x1, y1, 1.0)  # Expecting 100% accuracy for this simple case\n",
    "check_accuracy(test_model2, x2, y2, 1.0)  # Expecting 100% accuracy for this dataset\n",
    "check_accuracy(test_model3, x3, y3, 1.0)  # Expecting 100% accuracy for this dataset\n",
    "\n",
    "# Additional Tests for Edge Cases\n",
    "def test_empty_train():\n",
    "    with pytest.raises(ValueError):\n",
    "        test_model1.train(np.array([]), np.array([]))\n",
    "\n",
    "def test_empty_predict():\n",
    "    with pytest.raises(ValueError):\n",
    "        test_model1.predict(np.array([]))\n",
    "\n",
    "def test_accuracy_empty():\n",
    "    with pytest.raises(ValueError):\n",
    "        test_model1.accuracy(np.array([]), np.array([]))\n",
    "\n",
    "# Run additional edge case tests\n",
    "test_empty_train()\n",
    "test_empty_predict()\n",
    "test_accuracy_empty()\n",
    "\n",
    "# Print a message indicating the tests have completed\n",
    "print(\"All tests completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
